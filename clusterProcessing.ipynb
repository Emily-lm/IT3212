{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore, iqr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "data = pd.read_csv(\"smoking.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in data.dtypes.items():\n",
    "    print(f\"Column '{col}' has data type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_assign = ['hear_left', 'hear_right', 'SMK_stat_type_cd', 'urine_protein']\n",
    "data_num = data.select_dtypes(include='number')\n",
    "data_num.drop(columns_to_assign, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "data_cat = data.select_dtypes(exclude='number')\n",
    "data_cat[columns_to_assign] = data[columns_to_assign]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_all = {col: data_cat[col].unique() for col in data_cat.columns}\n",
    "\n",
    "print(unique_values_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualData(data, time):\n",
    "   for col in data.columns:\n",
    "        print(col)\n",
    "     # Plot Histogram of Z-scores\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Histogram plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(data[col], bins=100, edgecolor='k', alpha=0.7)\n",
    "        plt.title(f'Histogram of values {col}')\n",
    "        plt.xlabel('value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "\n",
    "        # Scatter Plot with Z-scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(time, data[col], alpha=0.7)\n",
    "        plt.title(f'Scatter Plot of values {col}')\n",
    "        plt.xlabel('time')\n",
    "        plt.ylabel('value')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualData(data=data_num, time=data_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normal_columns=['DBP', 'SBP', 'tot_chole', 'hemoglobin', 'weight', 'age', 'height', 'waistline']\n",
    "skewed_columns=[ 'BLDS', 'HDL_chole', 'LDL_chole', 'triglyceride', 'SGOT_AST', 'SGOT_ALT', 'gamma_GTP', 'serum_creatinine', 'sight_left', 'sight_right']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zscore(data):\n",
    "# Calculate Z-scores for each data point\n",
    "    z_scores = zscore(data)\n",
    "    # Define a threshold for identifying outliers\n",
    "    threshold = 3\n",
    "    # Identify outliers\n",
    "    outliers = (z_scores > threshold) | (z_scores < -threshold)\n",
    "    return outliers, z_scores\n",
    "\n",
    "z_outliers, zscores = Zscore(data_num[normal_columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualZscore_2(data, z_scores, outliers, threshold=3):\n",
    "    \"\"\"\n",
    "    Visualize actual values instead of Z-scores for a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Original DataFrame with actual values.\n",
    "    - z_scores: DataFrame with Z-scores for each column.\n",
    "    - outliers: DataFrame or boolean mask of outliers for each column.\n",
    "    - threshold: Z-score threshold for outliers.\n",
    "    \"\"\"\n",
    "    for col in z_scores.columns:\n",
    "        # Calculate mean and std deviation of the original data\n",
    "        mean = data[col].mean()\n",
    "        std = data[col].std()\n",
    "\n",
    "        # Convert Z-score thresholds back to actual value thresholds\n",
    "        lower_bound = mean - threshold * std\n",
    "        upper_bound = mean + threshold * std\n",
    "\n",
    "        # Plot Histogram of Actual Values\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Histogram plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(data[col], bins=100, edgecolor='k', alpha=0.7)\n",
    "        plt.axvline(lower_bound, color='r', linestyle='dashed', linewidth=1.5, label=f'Outlier Threshold ({lower_bound:.2f})')\n",
    "        plt.axvline(upper_bound, color='r', linestyle='dashed', linewidth=1.5, label=f'Outlier Threshold ({upper_bound:.2f})')\n",
    "        plt.title(f'Histogram of Actual Values ({col})')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "\n",
    "        # Scatter Plot with Actual Values\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(range(len(data[col])), data[col], c=['red' if outlier else 'blue' for outlier in outliers[col]], alpha=0.7)\n",
    "        plt.axhline(lower_bound, color='r', linestyle='dashed', linewidth=1.5, label=f'Lower Bound ({lower_bound:.2f})')\n",
    "        plt.axhline(upper_bound, color='r', linestyle='dashed', linewidth=1.5, label=f'Upper Bound ({upper_bound:.2f})')\n",
    "        plt.title(f'Scatter Plot of Actual Values ({col})')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        # Highlight outliers\n",
    "        \"\"\"for i, (value, is_outlier) in enumerate(zip(data[col], outliers[col])):\n",
    "            if is_outlier:\n",
    "                plt.text(i, value, f'{value:.2f}', fontsize=9, color='red', ha='left', va='bottom')\"\"\"\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualZscore(z_scores, outliers, threshold=3):\n",
    "    for col in z_scores.columns:\n",
    "        # Plot Histogram of Z-scores\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Histogram plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(z_scores[col], bins=100, edgecolor='k', alpha=0.7)\n",
    "        plt.axvline(threshold, color='r', linestyle='dashed', linewidth=1.5, label=f'Outlier Threshold (+{threshold})')\n",
    "        plt.axvline(-threshold, color='r', linestyle='dashed', linewidth=1.5, label=f'Outlier Threshold (-{threshold})')\n",
    "        plt.title(f'Histogram of Z-scores {col}')\n",
    "        plt.xlabel('Z-score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "\n",
    "        # Scatter Plot with Z-scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(range(len(z_scores[col])), z_scores[col], c=['red' if outlier else 'blue' for outlier in outliers[col]], alpha=0.7)\n",
    "        plt.axhline(threshold, color='r', linestyle='dashed', linewidth=1.5)\n",
    "        plt.axhline(-threshold, color='r', linestyle='dashed', linewidth=1.5)\n",
    "        plt.title(f'Scatter Plot of Z-scores {col}')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Z-score')\n",
    "        plt.grid(True)\n",
    "\n",
    "        \n",
    "\n",
    "        # Highlight outliers\n",
    "        for i, (score, is_outlier) in enumerate(zip(z_scores[col], outliers)):\n",
    "            if is_outlier:\n",
    "                plt.text(i, score, f'{score:.2f}', fontsize=9, color='red', ha='left', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR(df):\n",
    "    summary = df.describe()\n",
    "    Q1 = summary.loc['25%']\n",
    "    Q3 = summary.loc['75%']\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bounds = Q1 - 1.5 * IQR\n",
    "    upper_bounds = Q3 + 1.5 * IQR\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        lower_bound = lower_bounds[feature]\n",
    "        upper_bound = upper_bounds[feature]\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "        outliers_dict[feature] = outliers\n",
    "        \n",
    "    return outliers_dict, lower_bounds, upper_bounds\n",
    "\n",
    "iqr, lower_bound, upper_bound = IQR(data_num[skewed_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualIQR(data, lower_bound, upper_bound):\n",
    "    for col in data.columns:\n",
    "        # Plot Histogram of Z-scores\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Histogram plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(data[col], bins=100, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "        plt.axvline(x=lower_bound[col], color='r', linestyle='--', label=f'Lower Bound ({lower_bound[col]})')\n",
    "        plt.axvline(x=upper_bound[col], color='g', linestyle='--', label=f'Upper Bound ({upper_bound[col]})')\n",
    "\n",
    "        # Customize plot\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.yscale('log')\n",
    "        plt.title(f'Histogram with IQR Boundaries {col}')\n",
    "        plt.legend()\n",
    "\n",
    "        # Scatter Plot with Z-scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(range(len(data[col])), data[col], label='Data Points', color='b')\n",
    "        plt.axhline(y=lower_bound[col], color='r', linestyle='--', label=f'Lower Bound ({lower_bound[col]})')\n",
    "        plt.axhline(y=upper_bound[col], color='g', linestyle='--', label=f'Upper Bound ({upper_bound[col]})')\n",
    "\n",
    "        # Marking outliers\n",
    "        outliers = data[(data[col] < lower_bound[col]) | (data[col] > upper_bound[col])]\n",
    "        plt.scatter(np.where((data[col] < lower_bound[col]) | (data[col] > upper_bound[col]))[0], outliers[col], color='r', label='Outliers')\n",
    "\n",
    "        # Customize plot\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'Scatter Plot with IQR Boundaries and Outliers {col}')\n",
    "        plt.legend()\n",
    "        \n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualIQR(data=data_num[skewed_columns], lower_bound=lower_bound, upper_bound=upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualZscore(z_scores=zscores, outliers=z_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualZscore_2(data=data_num[normal_columns], z_scores=zscores, outliers=z_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling outliers\n",
    "data = data[data['SGOT_ALT'] >= 3]\n",
    "data = data[data['SGOT_AST'] >= 3] \n",
    "data = data[data['SGOT_ALT'] <= 1000] \n",
    "data = data[data['SGOT_AST'] <= 1000]  \n",
    "data = data[data['BLDS'] >= 40] \n",
    "data = data[data['BLDS'] <= 400] \n",
    "data = data[data['HDL_chole'] >= 16] \n",
    "data = data[data['waistline'] >= 30] \n",
    "data = data[data['waistline'] <= 200] \n",
    "data = data[data['serum_creatinine'] >= 0.25] \n",
    "data = data[data['tot_chole'] >= 80]\n",
    "data = data[data['DBP'] >= 46.38] \n",
    "data = data[data['SBP'] >= 78.8] \n",
    "data = data[data['hemoglobin'] >= 9.48] \n",
    "\n",
    "data['gamma_GTP'] = np.where(data['gamma_GTP'] > 400, 400, data['gamma_GTP'])  # Cap high values\n",
    "data['triglyceride'] = np.where(data['triglyceride'] > 500, 500, data['triglyceride'])\n",
    "data['HDL_chole'] = np.where(data['HDL_chole'] > 96, 96, data['HDL_chole'])\n",
    "data['LDL_chole'] = np.where(data['LDL_chole'] > 204, 204, data['LDL_chole'])\n",
    "data['sight_left'] = np.where(data['sight_left'] > 1.95, 1.95, data['sight_left'])\n",
    "data['sight_right'] = np.where(data['sight_right'] > 1.95, 1.95, data['sight_right'])\n",
    "data['serum_creatinine'] = np.where(data['serum_creatinine'] > 20, 20, data['serum_creatinine'])\n",
    "data['tot_chole'] = np.where(data['tot_chole'] > 500, 500, data['tot_chole'])\n",
    "data['DBP'] = np.where(data['DBP'] > 105.72, 105.72, data['DBP'])\n",
    "data['SBP'] = np.where(data['SBP'] > 166, 166, data['SBP'])\n",
    "data['hemoglobin'] = np.where(data['hemoglobin'] > 18.98, 18.98, data['hemoglobin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = data.select_dtypes(include='number')\n",
    "data_num.drop(columns_to_assign, axis=1, inplace=True)\n",
    "visualData(data=data_num, time=data_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data transformation\n",
    "\n",
    "#Feature scaling\n",
    "\n",
    "skew_features = ['gamma_GTP', 'SGOT_ALT', 'SGOT_AST', 'triglyceride']\n",
    "reflect_features=['sight_left', 'sight_right']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a copy of the original data\n",
    "data_transformed = data_num.copy()\n",
    "\n",
    "# Reflect and log-transform reflect_features\n",
    "for feature in reflect_features:\n",
    "    if data_transformed[feature].min() >= 0:\n",
    "        # Reflect the data\n",
    "        data_transformed[feature] = data_transformed[feature].max() + 1 - data_transformed[feature]\n",
    "        # Apply log transformation\n",
    "        data_transformed[feature] = np.log1p(data_transformed[feature])\n",
    "    else:\n",
    "        print(f\"Feature {feature} has negative values and cannot be reflected.\")\n",
    "\n",
    "# Log-transform skew_features\n",
    "for feature in skew_features:\n",
    "    data_transformed[feature] = np.log1p(data_transformed[feature])\n",
    "\n",
    "# Apply Min-Max Scaling to the entire DataFrame\n",
    "scaler = MinMaxScaler()\n",
    "data_transformed_scaled = scaler.fit_transform(data_transformed)\n",
    "\n",
    "# Convert the scaled NumPy array back to a DataFrame (optional)\n",
    "data_transformed_scaled = pd.DataFrame(data_transformed_scaled, columns=data_transformed.columns)\n",
    "\n",
    "# Check results\n",
    "data_transformed_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ord_features = ['hear_right', 'hear_left', 'urine_protein', 'SMK_stat_type_cd']\n",
    "notord_features=['sex', 'DRK_YN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "for feature in ord_features:\n",
    "    le = LabelEncoder()\n",
    "    data_cat[feature] = le.fit_transform(data_cat[feature])\n",
    "    label_encoders[feature] = le  # Save the encoder for later inverse transformation if needed\n",
    "\n",
    "# One-Hot Encoding for Non-Ordinal Features\n",
    "data_cat = pd.get_dummies(data_cat, columns=notord_features, drop_first=True)  # drop_first avoids multicollinearity\n",
    "data_cat = data_cat.astype(int)\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualData(data=data_transformed_scaled, time=data_transformed_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid division by zero by replacing zeros in the denominator\n",
    "epsilon = 1e-10  # A small constant to prevent division by zero\n",
    "\n",
    "# Waist-to-Weight Ratio\n",
    "data_transformed_scaled['waist_to_weight_ratio'] = (\n",
    "    data_transformed_scaled['waistline'] / (data_transformed_scaled['weight'] + epsilon)\n",
    ")\n",
    "\n",
    "# Pulse Pressure\n",
    "data_transformed_scaled['pulse_pressure'] = (\n",
    "    data_transformed_scaled['SBP'] - data_transformed_scaled['DBP']\n",
    ")\n",
    "\n",
    "# Cholesterol Ratio\n",
    "data_transformed_scaled['cholesterol_ratio'] = (\n",
    "    data_transformed_scaled['tot_chole'] / (data_transformed_scaled['HDL_chole'] + epsilon)\n",
    ")\n",
    "\n",
    "# AST/ALT Ratio\n",
    "data_transformed_scaled['ast_alt_ratio'] = (\n",
    "    data_transformed_scaled['SGOT_AST'] / (data_transformed_scaled['SGOT_ALT'] + epsilon)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat_aligned = data_cat.loc[data_transformed_scaled.index]\n",
    "\n",
    "# Combine the aligned data\n",
    "data_new = pd.concat([data_cat_aligned, data_transformed_scaled], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_new['SMK_stat_type_cd']\n",
    "data_new.drop('SMK_stat_type_cd', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(\"Are there NaN values?\")\n",
    "print(data_new.isna().sum())\n",
    "\n",
    "# Check for Infinite values\n",
    "print(\"Are there infinite values?\")\n",
    "print(np.isinf(data_new).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(data_new)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])\n",
    "\n",
    "print(pca_df.head())\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(\"Explained Variance by Component:\", explained_variance)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_variance)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit(data_new)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.scatter(x=[i+1 for i in range(len(df_pca.explained_variance_ratio_))], y=df_pca.explained_variance_ratio_, s=200, alpha=0.75, c='red')\n",
    "plt.grid(True)\n",
    "plt.title(\"Explained variance ratio of the \\nfitted principal component vector\\n\")\n",
    "plt.xlabel(\"Principal components\", fontsize=15)\n",
    "plt.xticks([i+1 for i in range(len(df_pca.explained_variance_ratio_))], fontsize=13)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylabel(\"Varaiance ratio\", fontsize=15)\n",
    "# plt.show()\n",
    "\n",
    "df_trans = pca.transform(data_new)\n",
    "df_trans = pd.DataFrame(data=df_trans)\n",
    "df_combined = pd.concat([data_new, df_trans, y], axis=1)\n",
    "# print(df_combined.head(10))\n",
    "columns1=df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(data_new)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "pca_minmax_scaled = scaler.fit_transform(principal_components)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pca_minmax_df = pd.DataFrame(pca_minmax_scaled, columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_minmax_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc = pd.concat([pca_minmax_df, data_new], axis=1)\n",
    "data_test = pd.concat([data_proc, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction\n",
    "import seaborn as sns\n",
    "correlation_matrix = data_test.corr()\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "X = data_test[['PC1', 'PC2', 'PC3', 'sex_Male', 'DRK_YN_Y']]  # Feature columns\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the splits\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Testing Features Shape:\", X_test.shape)\n",
    "print(\"Training Target Shape:\", y_train.shape)\n",
    "print(\"Testing Target Shape:\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('train/X_train_cluster.csv', index=False)\n",
    "y_train.to_csv('train/y_train_cluster.csv', index=False)\n",
    "\n",
    "X_test.to_csv('test/X_test_cluster.csv', index=False)\n",
    "y_test.to_csv('test/y_test_cluster.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
