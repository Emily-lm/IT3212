{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick any image based dataset from the list, implement the preprocessing and justify the\n",
    "preprocessing steps, extract features and justify the methods used, select features and justify the\n",
    "methods used. Some of this is done already in one of the previous assignments. You can reuse\n",
    "things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_image(image_path, downsample_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Convert image to grayscale, resize, and normalize pixel values.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert('L')  # Grayscale conversion\n",
    "    image = image.resize(downsample_size)        # Resizing\n",
    "    image_array = np.array(image, dtype=np.float32)\n",
    "    normalized_image = image_array / 255.0       # Normalization\n",
    "    return normalized_image\n",
    "\n",
    "# Load and preprocess images\n",
    "image_dir = 'vehicles'  # Path to the vehicles dataset\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Assuming images are organized in directories per vehicle type\n",
    "vehicle_labels = os.listdir(image_dir)\n",
    "\n",
    "for vehicle_label in vehicle_labels:\n",
    "    vehicle_dir = os.path.join(image_dir, vehicle_label)\n",
    "    if os.path.isdir(vehicle_dir):  # Ensure it's a directory\n",
    "        image_files = os.listdir(vehicle_dir)\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(vehicle_dir, image_file)\n",
    "            if os.path.isfile(image_path):\n",
    "                image = preprocess_image(image_path)\n",
    "                images.append(image)\n",
    "                labels.append(vehicle_label)\n",
    "            else:\n",
    "                print(f\"File not found: {image_path}\")\n",
    "    else:\n",
    "        print(f\"Directory not found: {vehicle_dir}\")\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Display sample images\n",
    "def display_sample_images(images, labels, label_encoder):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(1, 6):\n",
    "        idx = np.random.randint(0, len(images))\n",
    "        plt.subplot(1, 5, i)\n",
    "        plt.imshow(images[idx].reshape(64, 64), cmap='gray')\n",
    "        plt.title(label_encoder.inverse_transform([labels[idx]])[0])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle('Sample Images from Dataset')\n",
    "    plt.show()\n",
    "\n",
    "# Encode vehicle labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Display sample images\n",
    "display_sample_images(images, labels_encoded, label_encoder)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels_encoded, test_size=0.2, random_state=42, stratify=labels_encoded\n",
    ")\n",
    "\n",
    "# Flatten images for ML algorithms\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_flat_scaled = scaler.fit_transform(X_train_flat)\n",
    "X_test_flat_scaled = scaler.transform(X_test_flat)\n",
    "\n",
    "# Feature Extraction using PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train_flat_scaled)\n",
    "\n",
    "# Explained Variance - Individual and Cumulative\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Focus on the first 100 components for plotting\n",
    "max_components_to_plot = 100\n",
    "explained_variance_ratio_subset = explained_variance_ratio[:max_components_to_plot]\n",
    "cumulative_variance_subset = cumulative_variance[:max_components_to_plot]\n",
    "\n",
    "# Plot individual and cumulative variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(explained_variance_ratio_subset) + 1), explained_variance_ratio_subset, alpha=0.6, align='center',\n",
    "        label='Individual Explained Variance')\n",
    "plt.step(range(1, len(cumulative_variance_subset) + 1), cumulative_variance_subset, where='mid',\n",
    "         label='Cumulative Explained Variance', color='red')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.title('PCA Explained Variance (Top 100 Components)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Select number of components explaining 95% variance\n",
    "k = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Number of components explaining 95% variance: {k}\")\n",
    "\n",
    "# Transform data using PCA\n",
    "pca = PCA(n_components=k)\n",
    "X_train_pca = pca.fit_transform(X_train_flat_scaled)\n",
    "X_test_pca = pca.transform(X_test_flat_scaled)\n",
    "\n",
    "\n",
    "# Basic ML Algorithm: K-Nearest Neighbors\n",
    "start_time = time.time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "y_pred_knn = knn.predict(X_test_pca)\n",
    "knn_time = time.time() - start_time\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy: {knn_accuracy*100:.2f}%, Time: {knn_time:.2f}s\")\n",
    "print(\"KNN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# Confusion Matrix for KNN\n",
    "disp_knn = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred_knn, display_labels=label_encoder.classes_, xticks_rotation='vertical'\n",
    ")\n",
    "plt.title('KNN Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Advanced ML Algorithm: Support Vector Machine\n",
    "start_time = time.time()\n",
    "svm = SVC(kernel='rbf', gamma='scale')\n",
    "svm.fit(X_train_pca, y_train)\n",
    "y_pred_svm = svm.predict(X_test_pca)\n",
    "svm_time = time.time() - start_time\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_accuracy*100:.2f}%, Time: {svm_time:.2f}s\")\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "disp_svm = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred_svm, display_labels=label_encoder.classes_, xticks_rotation='vertical'\n",
    ")\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# CNN Model\n",
    "start_time = time.time()\n",
    "X_train_cnn = X_train.reshape(-1, 64, 64, 1)\n",
    "X_test_cnn = X_test.reshape(-1, 64, 64, 1)\n",
    "y_train_cnn = to_categorical(y_train, num_classes=len(label_encoder.classes_))\n",
    "y_test_cnn = to_categorical(y_test, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.3),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    epochs=20, batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate CNN\n",
    "y_pred_cnn = model.predict(X_test_cnn)\n",
    "y_pred_cnn_classes = np.argmax(y_pred_cnn, axis=1)\n",
    "cnn_time = time.time() - start_time\n",
    "cnn_accuracy = accuracy_score(y_test, y_pred_cnn_classes)\n",
    "print(f\"CNN Accuracy: {cnn_accuracy*100:.2f}%\")\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Training and Validation Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['accuracy'], 'bo-', label='Training accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'ro-', label='Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Training and Validation Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['loss'], 'bo-', label='Training loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'ro-', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the results\n",
    "plot_training_history(history)\n",
    "\n",
    "# Confusion Matrix for CNN\n",
    "disp_cnn = ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred_cnn_classes, display_labels=label_encoder.classes_, xticks_rotation='vertical'\n",
    ")\n",
    "plt.title('CNN Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Comparison of Results\n",
    "print(\"\\nComparison of Algorithms:\")\n",
    "print(f\"KNN - Accuracy: {knn_accuracy*100:.2f}%, Time: {knn_time:.2f}s\")\n",
    "print(f\"SVM - Accuracy: {svm_accuracy*100:.2f}%, Time: {svm_time:.2f}s\")\n",
    "print(f\"CNN - Accuracy: {cnn_accuracy*100:.2f}%,Time: {cnn_time:.2f}s\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
