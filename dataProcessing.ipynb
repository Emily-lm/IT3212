{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading of datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore, iqr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "data_weather = pd.read_csv(\"spanish-cities-energy-consumption/weather_features.csv\")\n",
    "data_generation = pd.read_csv(\"spanish-cities-energy-consumption/energy_dataset.csv\")\n",
    "data_weather['dt_iso'] = pd.to_datetime(data_weather['dt_iso'])\n",
    "data_generation['time'] = pd.to_datetime(data_generation['time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= 35064*5\n",
    "b= 178396 -a\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = data_weather[data_weather['dt_iso'].duplicated(keep=False)]  # keep=False to include the first occurrence as well\n",
    "\n",
    "# Sort the duplicates based on 'dt_iso' in ascending order (default)\n",
    "duplicates_sorted = duplicates[['dt_iso', 'city_name']].sort_values(by='dt_iso')\n",
    "\n",
    "# Display the sorted duplicates\n",
    "#print(duplicates_sorted)\n",
    "print(pd.concat([duplicates_sorted.head(), duplicates_sorted.tail()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generation.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generation.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generation.drop(columns=[\"generation fossil coal-derived gas\",\n",
    "                              \"generation hydro pumped storage aggregated\", \n",
    "                              \"generation fossil oil shale\",\n",
    "                              \"generation fossil peat\", \n",
    "                              \"generation geothermal\", \n",
    "                              \"generation marine\", \n",
    "                              \"generation wind offshore\",\n",
    "                              \"forecast wind offshore eday ahead\" ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in data_weather.dtypes.items():\n",
    "    print(f\"Column '{col}' has data type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, dtype in data_generation.dtypes.items():\n",
    "    print(f\"Column '{col}' has data type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_weather.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_generation.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the numerical features from the categorical features\n",
    "weather_num = data_weather.select_dtypes(include='number')\n",
    "weather_num.drop('weather_id', axis=1, inplace=True)\n",
    "weather_cat = data_weather.select_dtypes(exclude='number').drop('dt_iso', axis=1)\n",
    "weather_cat['weather_id']=data_weather['weather_id']\n",
    "generation_num = data_generation.select_dtypes(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the unique values of categorical features to check for outliers\n",
    "unique_values_all = {col: weather_cat[col].unique() for col in weather_cat.columns}\n",
    "\n",
    "print(unique_values_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualData(data, time):\n",
    "   for col in data.columns:\n",
    "        print(col)\n",
    "     # Plot Histogram of Z-scores\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Histogram plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(data[col], bins=100, edgecolor='k', alpha=0.7)\n",
    "        plt.title(f'Histogram of values {col}')\n",
    "        plt.xlabel('value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "\n",
    "        # Scatter Plot with Z-scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(time, data[col], alpha=0.7)\n",
    "        plt.title(f'Scatter Plot of values over time {col}')\n",
    "        plt.xlabel('time')\n",
    "        plt.ylabel('value')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualData(data=weather_num, time=data_weather['dt_iso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualData(data=generation_num, time=data_generation['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather dataset\n",
    "normal_weather_columns=[\"temp\", 'temp_min', 'temp_max', 'pressure']\n",
    "skewed_weather_columns=['humidity', 'wind_speed', 'wind_deg', 'rain_1h', 'rain_3h', 'snow_3h', 'clouds_all']\n",
    "\n",
    "#energy dataset\n",
    "\n",
    "normal_energy_columns=['generation biomass', 'generation fossil hard coal', 'generation fossil oil',\n",
    "                        'generation other', 'generation other renewable', 'total load forecast','total load actual',\n",
    "                         'price day ahead', 'price actual']\n",
    "skewed_energy_columns=['generation fossil brown coal/lignite', 'generation fossil gas','generation hydro pumped storage consumption',\n",
    "                        'generation hydro run-of-river and poundage', 'generation hydro water reservoir', 'generation nuclear',\n",
    "                         'generation solar', 'generation waste', 'generation wind onshore',\n",
    "                           'forecast solar day ahead', 'forecast wind onshore day ahead']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_sum = data_generation.isnull().sum()\n",
    "missing_values_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "columns_to_median_impute=['generation fossil brown coal/lignite', 'generation fossil gas','generation hydro pumped storage consumption',\n",
    "                        'generation hydro run-of-river and poundage', 'generation hydro water reservoir', 'generation nuclear',\n",
    "                         'generation solar', 'generation waste', 'generation wind onshore']\n",
    "\n",
    "columns_to_mean_impute=['generation biomass', 'generation fossil hard coal', 'generation fossil oil',\n",
    "                        'generation other', 'generation other renewable', 'total load actual']\n",
    "\n",
    "generation_num[columns_to_median_impute] = generation_num[columns_to_median_impute].fillna(generation_num.median(numeric_only=True))\n",
    "generation_num[columns_to_mean_impute] = generation_num[columns_to_mean_impute].fillna(generation_num.mean(numeric_only=True))\n",
    "\n",
    "\n",
    "print(generation_num.isna().sum())\n",
    "\n",
    "print(\"Number of missing values now is:\", pd.isnull(generation_num).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zscore(data):\n",
    "# Calculate Z-scores for each data point\n",
    "    z_scores = zscore(data)\n",
    "    # Define a threshold for identifying outliers\n",
    "    threshold = 3\n",
    "    # Identify outliers\n",
    "    outliers = (z_scores > threshold) | (z_scores < -threshold)\n",
    "    return outliers, z_scores\n",
    "\n",
    "weather_outliers, weather_zscores = Zscore(weather_num[normal_weather_columns])\n",
    "generation_outliers, generation_zscore = Zscore(generation_num[normal_energy_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualZscore(z_scores, outliers, threshold=3):\n",
    "    for col in z_scores.columns:\n",
    "        # Plot Histogram of Z-scores\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Histogram plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(z_scores[col], bins=100, edgecolor='k', alpha=0.7)\n",
    "        plt.axvline(threshold, color='r', linestyle='dashed', linewidth=1.5, label=f'Outlier Threshold (+{threshold})')\n",
    "        plt.axvline(-threshold, color='r', linestyle='dashed', linewidth=1.5, label=f'Outlier Threshold (-{threshold})')\n",
    "        plt.title(f'Histogram of Z-scores {col}')\n",
    "        plt.xlabel('Z-score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "\n",
    "        # Scatter Plot with Z-scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(range(len(z_scores[col])), z_scores[col], c=['red' if outlier else 'blue' for outlier in outliers[col]], alpha=0.7)\n",
    "        plt.axhline(threshold, color='r', linestyle='dashed', linewidth=1.5)\n",
    "        plt.axhline(-threshold, color='r', linestyle='dashed', linewidth=1.5)\n",
    "        plt.title(f'Scatter Plot of Z-scores {col}')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Z-score')\n",
    "        plt.grid(True)\n",
    "\n",
    "        \n",
    "\n",
    "        # Highlight outliers\n",
    "        for i, (score, is_outlier) in enumerate(zip(z_scores[col], outliers)):\n",
    "            if is_outlier:\n",
    "                plt.text(i, score, f'{score:.2f}', fontsize=9, color='red', ha='left', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def IQR(df):\n",
    "    summary = df.describe()\n",
    "    Q1 = summary.loc['25%']\n",
    "    Q3 = summary.loc['75%']\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bounds = Q1 - 1.5 * IQR\n",
    "    upper_bounds = Q3 + 1.5 * IQR\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        lower_bound = lower_bounds[feature]\n",
    "        upper_bound = upper_bounds[feature]\n",
    "        \n",
    "        # Identify outliers\n",
    "        outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "        outliers_dict[feature] = outliers\n",
    "        \n",
    "    return outliers_dict, lower_bounds, upper_bounds\n",
    "\n",
    "weather_iqr, weather_lower_bound, weather_upper_bound = IQR(weather_num[skewed_weather_columns])\n",
    "generation_iqr, generation_lower_bound, generation_upper_bound = IQR(generation_num[skewed_energy_columns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualIQR(data, lower_bound, upper_bound):\n",
    "    for col in data.columns:\n",
    "        # Plot Histogram of Z-scores\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Histogram plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(data[col], bins=100, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "        plt.axvline(x=lower_bound[col], color='r', linestyle='--', label=f'Lower Bound ({lower_bound[col]})')\n",
    "        plt.axvline(x=upper_bound[col], color='g', linestyle='--', label=f'Upper Bound ({upper_bound[col]})')\n",
    "\n",
    "        # Customize plot\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.yscale('log')\n",
    "        plt.title(f'Histogram with IQR Boundaries {col}')\n",
    "        plt.legend()\n",
    "\n",
    "        # Scatter Plot with Z-scores\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(range(len(data[col])), data[col], label='Data Points', color='b')\n",
    "        plt.axhline(y=lower_bound[col], color='r', linestyle='--', label=f'Lower Bound ({lower_bound[col]})')\n",
    "        plt.axhline(y=upper_bound[col], color='g', linestyle='--', label=f'Upper Bound ({upper_bound[col]})')\n",
    "\n",
    "        # Marking outliers\n",
    "        outliers = data[(data[col] < lower_bound[col]) | (data[col] > upper_bound[col])]\n",
    "        plt.scatter(np.where((data[col] < lower_bound[col]) | (data[col] > upper_bound[col]))[0], outliers[col], color='r', label='Outliers')\n",
    "\n",
    "        # Customize plot\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'Scatter Plot with IQR Boundaries and Outliers {col}')\n",
    "        plt.legend()\n",
    "        \n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualIQR(data=weather_num[skewed_weather_columns], lower_bound=weather_lower_bound, upper_bound=weather_upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualIQR(data=generation_num[skewed_energy_columns], lower_bound=generation_lower_bound, upper_bound=generation_upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualZscore(z_scores=weather_zscores, outliers=weather_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualZscore(z_scores=generation_zscore, outliers=generation_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling of outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather dataset\n",
    "\n",
    "cap_weather_columns=[\"temp\", 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'rain_1h', 'snow_3h']\n",
    "remove_weather_columns=[]\n",
    "transform_weather_columns=['rain_3h']\n",
    "\n",
    "#cap rain at a different threshold \n",
    "\n",
    "#energy dataset\n",
    "\n",
    "normal_energy_columns=['generation biomass', 'generation fossil hard coal', 'generation fossil oil',\n",
    "                        'generation other', 'generation other renewable', 'total load forecast','total load actual',\n",
    "                         'price day ahead', 'price actual']\n",
    "skewed_energy_columns=['generation fossil brown coal/lignite', 'generation fossil gas','generation hydro pumped storage consumption',\n",
    "                        'generation hydro run-of-river and poundage', 'generation hydro water reservoir', 'generation nuclear',\n",
    "                         'generation solar', 'generation waste', 'generation wind onshore',\n",
    "                           'forecast solar day ahead', 'forecast wind onshore day ahead']\n",
    "\n",
    "cap_energy_columns=['generation fossil oil', 'generation nuclear', 'generation other renewable','generation waste', 'generation wind onshore', 'generation biomass']\n",
    "#remove_energy_columns=['generation biomass']\n",
    "transform_energy_columns=['generation fossil gas','generation hydro pumped storage consumption','generation hydro water reservoir' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capping\n",
    "for col in weather_num[cap_weather_columns].columns:\n",
    "    if col in skewed_weather_columns:\n",
    "        if col == 'rain_1h':\n",
    "            weather_num[col] = np.clip(weather_num[col], weather_lower_bound[col], 3)\n",
    "        else:\n",
    "            weather_num[col] = np.clip(weather_num[col], weather_lower_bound[col], weather_upper_bound[col])\n",
    "    else:\n",
    "        mean = np.mean(weather_num[col])\n",
    "        std_dev = np.std(weather_num[col])\n",
    "        lower_bound = mean - (3 * std_dev)\n",
    "        upper_bound = mean + (3 * std_dev)\n",
    "        weather_num[col] = np.clip(weather_num[col], lower_bound, upper_bound)\n",
    "\n",
    "#log-transform\n",
    "for col in weather_num[transform_weather_columns].columns:\n",
    "    weather_num[col] = np.log1p(weather_num[col])\n",
    "\n",
    "#capping\n",
    "for col in generation_num[cap_energy_columns].columns:\n",
    "    if col in skewed_energy_columns:\n",
    "        generation_num[col] = np.clip(generation_num[col], generation_lower_bound[col], generation_upper_bound[col])\n",
    "    else:\n",
    "        mean = np.mean(generation_num[col])\n",
    "        std_dev = np.std(generation_num[col])\n",
    "        lower_bound = mean - (3 * std_dev)\n",
    "        upper_bound = mean + (3 * std_dev)\n",
    "        generation_num[col] = np.clip(generation_num[col], lower_bound, upper_bound)\n",
    "\n",
    "#log-transform\n",
    "for col in generation_num[transform_energy_columns].columns:\n",
    "    generation_num[col] = np.log1p(generation_num[col])\n",
    "\n",
    "#remove\n",
    "#print(generation_outliers[remove_energy_columns])\n",
    "\"\"\"true_indexes = generation_outliers[generation_outliers['generation biomass'] == True].index\n",
    "\n",
    "print(true_indexes)\n",
    "\n",
    "generation_num.drop(true_indexes, inplace=True)\n",
    "data_generation.drop(true_indexes, inplace=True)\n",
    "generation_num.reset_index(drop=True, inplace=True)\n",
    "data_generation.reset_index(drop=True, inplace=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualData(data=weather_num, time=data_weather['dt_iso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualData(data=generation_num, time=data_generation['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded= pd.get_dummies(weather_cat, columns=['weather_main', 'weather_description', 'weather_icon',\n",
    "       'weather_id'])\n",
    "df_encoded=df_encoded.drop(columns=['city_name'])\n",
    "\n",
    "df_encoded = df_encoded.astype(int)\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_cat = pd.concat([weather_cat['city_name'], df_encoded], axis=1)\n",
    "weather_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "#weather dataset\n",
    "norm_weather_columns=[\"temp\", 'temp_min', 'temp_max', 'pressure', 'snow_3h']\n",
    "skew_weather_columns=['humidity', 'wind_speed', 'rain_1h', 'wind_deg', 'rain_3h', 'clouds_all']\n",
    "modal_weather_columns=[]\n",
    "\n",
    "#cap rain at a different threshold \n",
    "\n",
    "#energy dataset\n",
    "\n",
    "\n",
    "norm_energy_columns=['generation fossil gas', 'generation fossil oil', 'price day ahead', 'price actual']\n",
    "#remove_energy_columns=['generation biomass']\n",
    "skew_energy_columns=['generation fossil brown coal/lignite', 'generation hydro pumped storage consumption', \n",
    "                     'generation hydro run-of-river and poundage', 'generation hydro water reservoir', 'generation solar',\n",
    "                     'generation waste', 'generation wind onshore', 'forecast solar day ahead', 'forecast wind onshore day ahead']\n",
    "modal_energy_columns=['generation biomass', 'generation fossil hard coal', 'generation nuclear', 'generation other', 'generation other renewable',\n",
    "                      'total load forecast','total load actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "x = weather_num[norm_weather_columns].to_numpy()\n",
    "standard_scaler = StandardScaler().fit(x)\n",
    "weather_num[norm_weather_columns] = standard_scaler.transform(x)\n",
    "x = generation_num[norm_energy_columns].to_numpy()\n",
    "standard_scaler = StandardScaler().fit(x)\n",
    "generation_num[norm_energy_columns] = standard_scaler.transform(x)\n",
    "\n",
    "x = weather_num[skew_weather_columns].to_numpy()\n",
    "robust_scaler = RobustScaler()\n",
    "weather_num[skew_weather_columns] = robust_scaler.fit_transform(x)\n",
    "x = generation_num[skew_energy_columns].to_numpy()\n",
    "robust_scaler = RobustScaler()\n",
    "generation_num[skew_energy_columns] = robust_scaler.fit_transform(x)\n",
    "\n",
    "x = generation_num[modal_energy_columns].to_numpy()\n",
    "quantile_transformer = QuantileTransformer(output_distribution=\"normal\")\n",
    "generation_num[modal_energy_columns]= quantile_transformer.fit_transform(x)\n",
    "\n",
    "visualData(data=weather_num, time=data_weather['dt_iso'])\n",
    "visualData(data=generation_num, time=data_generation['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather = pd.concat([data_weather['dt_iso'],weather_num, weather_cat], axis=1)\n",
    "combined_weather.head()\n",
    "combined_gen = pd.concat([data_generation['time'],generation_num], axis=1)\n",
    "combined_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather['dt_iso'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_to_find = pd.Timestamp('2015-01-01 00:00:00+01:00', tz='UTC+01:00')\n",
    "filtered_rows = combined_weather[combined_weather['dt_iso'] == timestamp_to_find]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(filtered_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = combined_weather.pivot_table(index='dt_iso', columns='city_name', aggfunc='first')\n",
    "\n",
    "# Flatten multi-level columns (from city_name)\n",
    "pivot_df.columns = [f\"{col[0]}_{col[1]}\" for col in pivot_df.columns]\n",
    "\n",
    "# Reset index to make 'dt_iso' a column\n",
    "pivot_df = pivot_df.reset_index()\n",
    "\n",
    "# Print the reshaped DataFrame\n",
    "pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(pivot_df, combined_gen, left_on='dt_iso', right_on='time', how='inner')\n",
    "merged_df= merged_df.drop(columns=['time'])\n",
    "# Print the merged dataframe\n",
    "print(merged_df.shape)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df.drop('total load actual', axis=1)  # Features (predictors)\n",
    "y = merged_df['total load actual']  # Target variable (what we want to predict)\n",
    "\n",
    "\"\"\"# Perform the time-based split, setting shuffle=False to maintain order\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=False\n",
    ")\"\"\"\n",
    "# First, split into training and temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Now split the temp into validation and test sets (50% each of the remaining 30%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Print the shape of the splits\n",
    "print(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape (X_val, y_val):\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set shape (X_test, y_test):\", X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('train/X_train.csv', index=False)\n",
    "y_train.to_csv('train/y_train.csv', index=False)\n",
    "\n",
    "X_val.to_csv('validation/X_val.csv', index=False)\n",
    "y_val.to_csv('validation/y_val.csv', index=False)\n",
    "\n",
    "X_test.to_csv('test/X_test.csv', index=False)\n",
    "y_test.to_csv('test/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = weather_num.drop('wind_deg', axis=1)  # Features\n",
    "y = weather_num['wind_deg']  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for dimensionality reduction\n",
    "scaler = StandardScaler()\n",
    "X = weather_num.drop('wind_deg', axis=1)  # Features\n",
    "y = weather_num['wind_deg']  \n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X)\n",
    "pca = PCA(n_components=len(X.columns))\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, alpha=0.7)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), cumulative_variance, c='r', marker='o')\n",
    "\n",
    "# Annotate cumulative variance on the plot\n",
    "for i, value in enumerate(cumulative_variance):\n",
    "    plt.text(i + 1, cumulative_variance[i], f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Explained Variance Ratio by Components')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.legend(['Cumulative Explained Variance', 'Explained Variance Ratio'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the difference in amount of features before and after PCA\n",
    "print(f'Number of features before PCA: {X.shape[1]}')\n",
    "print(f'Number of features after PCA: {X_pca.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data to CSV files\n",
    "merged_df.to_csv('preprocessed_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
